{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self,inputs,units,activation='relu',use_bias=True,kernel_initializer=None,bias_regularizer=False,dropout=False):\n",
    "        self.inputs = inputs\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "#         self.kernel_regularizer=kernel_regularizer\n",
    "        self.bias_regularizer=bias_regularizer\n",
    "        self.dropout = dropout\n",
    "        self.b = np.zeros((self.units,1))\n",
    "        self.inward = 0\n",
    "        self.z = 0\n",
    "        self.a = 0\n",
    "        self.dw = 0\n",
    "        self.db = 0\n",
    "        self.da = 0\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        #applying the sigmoid function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def initialize_weight(self):\n",
    "        if(self.activation == 'relu'):\n",
    "            self.w = np.random.rand(self.units,self.inputs) * np.sqrt(2.0/self.inputs)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.w = np.random.rand(self.units,self.inputs) * np.sqrt(1.0/self.inputs)\n",
    "        else:\n",
    "            self.w = np.random.rand(self.units,self.inputs) * np.sqrt(2.0/(self.units+self.inputs))\n",
    "            \n",
    "    def sigmoid_d(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        x = x - np.mean(x)\n",
    "#         print x\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum()\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        #applying the tanh function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh_d(self, x):\n",
    "        return (1 + x) * (1 - x)\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x) \n",
    "    \n",
    "    def relu_d(self,x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def update(self,alpha):\n",
    "#         print \"W = \",self.w\n",
    "#         print \"b = \",self.b\n",
    "#         print \"dW = \",self.dw\n",
    "#         print \"db = \",self.db\n",
    "        self.w += alpha * self.dw\n",
    "        self.b += alpha * self.db\n",
    "        \n",
    "    \n",
    "    def forward(self,inward):\n",
    "        inward = inward.reshape((inward.shape[0],1))\n",
    "        self.inward = inward\n",
    "        self.z = np.dot(self.w,inward) \n",
    "        self.a = self.z\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            self.a = self.sigmoid(self.a)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.a = self.tanh(self.a)\n",
    "        elif(self.activation == 'relu'):\n",
    "            self.a = self.relu(self.a)\n",
    "        elif(self.activation == 'softmax'):\n",
    "            self.a = self.softmax(self.a)\n",
    "        else:\n",
    "            pass\n",
    "        return self.a\n",
    "            \n",
    "    def backward(self,a):\n",
    "#         print a.shape\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            self.dz = np.multiply(a,self.sigmoid(self.z))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.dz = np.multiply(a,self.tanh(self.z))\n",
    "        elif(self.activation == 'relu'):\n",
    "            self.dz = np.multiply(a,self.relu(self.z))\n",
    "        elif(self.activation == 'softmax'):\n",
    "            self.dz = a\n",
    "        else:\n",
    "            self.dz = 0 * a\n",
    "            \n",
    "        self.dw = np.dot(self.dz, self.inward.T)\n",
    "        self.db = np.sum(self.dz, axis=1, keepdims=True)\n",
    "        next_a = np.dot(self.w.T, self.dz)\n",
    "        return next_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self,data,data_label,number_of_class,alpha=0.1,accuracy=0.9):\n",
    "        self.layers = []\n",
    "        self.alpha = alpha\n",
    "        self.accuracy = accuracy\n",
    "        self.data = data\n",
    "        self.data_label = data_label \n",
    "        self.number_of_class = number_of_class\n",
    "        self.cm = np.zeros((self.number_of_class,self.number_of_class),dtype=int)\n",
    "        self.eplison = 10**(-4)\n",
    "        \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def train(self):\n",
    "        i = 0\n",
    "        while(i < 10):\n",
    "            print i\n",
    "            self.cm = np.zeros((self.number_of_class,self.number_of_class),dtype=int)\n",
    "            self.do_epoch()\n",
    "            self.get_accuracy()\n",
    "            i+=1\n",
    "#             print self.layers[0].dw\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        print self.cm\n",
    "        print np.trace(self.cm)/(1.0*np.sum(self.cm))\n",
    "\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update(self.alpha)\n",
    "    \n",
    "    def get_cost(self,y,y_label):\n",
    "        \n",
    "#         print y_label,y\n",
    "        if(self.number_of_class > 2):\n",
    "            y_label = y_label.reshape((y_label.shape[0],1))\n",
    "            return (y_label - y)\n",
    "        else:\n",
    "            if (y_label == 1):\n",
    "                return -1*np.log(y + self.eplison)\n",
    "            else:\n",
    "                return -1*np.log(1 - y + self.eplison)\n",
    "    \n",
    "    def predict(self,sample):\n",
    "        for layer in self.layers:\n",
    "            sample = layer.forward(sample)\n",
    "        return sample.argmax()\n",
    "    \n",
    "    def do_epoch(self):\n",
    "        total_cost = 0\n",
    "        for j in range(len(self.data)):\n",
    "            y_label = self.data_label[j]\n",
    "            x = self.data[j]\n",
    "            y = self.forprop(x)\n",
    "            loss = self.get_cost(y,y_label)\n",
    "#             print y_label,y\n",
    "#             print y_label.argmax(),y.argmax()\n",
    "#             print self.number_of_class\n",
    "            if(self.number_of_class > 2):\n",
    "                self.cm[y_label.argmax()][y.argmax()] += 1\n",
    "#                 print self.cm\n",
    "            else:\n",
    "                if(y>0.5):\n",
    "                    self.cm[y_label][1] += 1\n",
    "                else:\n",
    "                    self.cm[y_label][0] += 1\n",
    "            self.backprop(loss)\n",
    "            self.update()\n",
    "#             total_cost += loss\n",
    "            \n",
    "    def forprop(self,sample):\n",
    "        for layer in self.layers:\n",
    "#             print sample\n",
    "            sample = layer.forward(sample)\n",
    "                  \n",
    "        return sample       \n",
    "    \n",
    "    def backprop(self,loss):\n",
    "        sample = loss\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            sample = self.layers[i].backward(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train_label),(x_test, y_test_label) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape([60000,784])\n",
    "x_test = x_test.reshape([10000,784])\n",
    "y_train = np.zeros((y_train_label.shape[0], 10))\n",
    "y_train[np.arange(y_train_label.shape[0]), y_train_label] = 1\n",
    "y_test = np.zeros((y_test_label.shape[0], 10))\n",
    "y_test[np.arange(y_test_label.shape[0]), y_test_label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[5497    1   85   40   19   79   70   39   76   17]\n",
      " [  38 6045  177  131   36   34   66   23  173   19]\n",
      " [ 102   71 5030  128  120   36  179   94  160   38]\n",
      " [ 104   49  204 5105   19  250   47   79  217   57]\n",
      " [  26   25  120   24 4991   17  137   44   64  394]\n",
      " [ 213   37   90  349   65 4057  151   63  316   80]\n",
      " [ 103   34  181   30  101  104 5269   13   78    5]\n",
      " [  76   49  120   55  102   35   27 5480   75  246]\n",
      " [  77  149  202  232   48  192   82   35 4725  109]\n",
      " [  69   27   89  108  403   73   32  294  134 4720]]\n",
      "0.84865\n",
      "1\n",
      "[[5757    0   17    7    6   15   49   19   34   19]\n",
      " [   1 6585   36   33   10    3    7   16   42    9]\n",
      " [  40   29 5559   61   43   10   48   61   92   15]\n",
      " [  11   21  103 5680    4  120    8   42  110   32]\n",
      " [   7   14   19    3 5486    2   75   18   19  199]\n",
      " [  54   14   24  129   24 4949   88   13   74   52]\n",
      " [  43   14   19    4   28   57 5725    0   27    1]\n",
      " [  20   31   60   25   49   11    2 5968   15   84]\n",
      " [  19   64   64  105   27   68   46   12 5395   51]\n",
      " [  28   20   11   65  136   34    2  110   65 5478]]\n",
      "0.9430333333333333\n",
      "2\n",
      "[[5795    0   16    5    3    9   40    9   27   19]\n",
      " [   1 6592   38   29   11    2    4   12   41   12]\n",
      " [  39   30 5638   53   36    7   27   51   68    9]\n",
      " [   3   11   82 5781    1   94    4   36   91   28]\n",
      " [   6   12   17    2 5565    4   58   14   11  153]\n",
      " [  33    8   17  122   18 5049   66   12   54   42]\n",
      " [  31   10   15    1   31   47 5763    0   20    0]\n",
      " [  12   26   59   21   37    5    2 6009   15   79]\n",
      " [  20   43   42   66   16   59   26   11 5510   58]\n",
      " [  21   15    7   44  113   28    3   89   52 5577]]\n",
      "0.95465\n",
      "3\n",
      "[[5826    0   10    3    5    5   26    7   22   19]\n",
      " [   1 6615   30   24   13    3    3   10   32   11]\n",
      " [  29   26 5711   44   27   10   20   40   44    7]\n",
      " [   3    7   73 5817    1   88    2   34   80   26]\n",
      " [   8   12   17    1 5610    4   35   18    8  129]\n",
      " [  17    6   14  102   16 5121   62   10   40   33]\n",
      " [  27    7    8    1   26   39 5793    0   17    0]\n",
      " [   8   21   48   15   26    7    1 6055   11   73]\n",
      " [  14   32   36   62   21   54   19   11 5555   47]\n",
      " [  19   13    3   42  100   25    2   86   40 5619]]\n",
      "0.9620333333333333\n",
      "4\n",
      "[[5831    0   12    3    3    7   19    9   21   18]\n",
      " [   1 6622   29   21   10    3    2   13   30   11]\n",
      " [  25   23 5722   50   24    9   18   36   43    8]\n",
      " [   4    8   74 5843    2   75    3   28   67   27]\n",
      " [   6    8   16    1 5641    4   34   16   12  104]\n",
      " [  17    7   10   93   11 5149   55   11   38   30]\n",
      " [  20    7    8    1   22   39 5806    1   14    0]\n",
      " [  11   17   47   13   20    4    0 6091    6   56]\n",
      " [  12   31   33   58   17   44   19   12 5582   43]\n",
      " [  20   13    3   37   95   20    2   65   39 5655]]\n",
      "0.9657\n",
      "5\n",
      "[[5832    0   12    3    6    9   21    5   16   19]\n",
      " [   2 6631   24   20   11    4    4   18   22    6]\n",
      " [  25   20 5746   41   22    8   18   42   32    4]\n",
      " [   4    7   68 5863    1   74    3   24   63   24]\n",
      " [   4   11   14    1 5646    2   30   14   12  108]\n",
      " [  14    5    4   73   10 5183   56   10   38   28]\n",
      " [  21    9    7    2   20   36 5800    1   22    0]\n",
      " [   9   20   44   12   23    4    0 6095    7   51]\n",
      " [  10   32   31   49   14   41   17   10 5610   37]\n",
      " [  19    8    4   29   92   23    0   58   34 5682]]\n",
      "0.9681333333333333\n",
      "6\n",
      "[[5813    2   10    5    4   11   25    7   26   20]\n",
      " [   1 6637   23   15    7    4    8   13   23   11]\n",
      " [  12   21 5771   31   20    8   19   40   31    5]\n",
      " [   6    5   65 5885    1   61    3   21   59   25]\n",
      " [   5    7   14    1 5662    1   28   12    9  103]\n",
      " [  22    9   10   72    8 5173   51   14   40   22]\n",
      " [  17   10    8    1   16   42 5800    0   24    0]\n",
      " [   4   20   36   13   18    8    0 6116    7   43]\n",
      " [  12   32   27   44   12   38   22    8 5623   33]\n",
      " [  18   11    3   33   82   16    2   51   34 5699]]\n",
      "0.96965\n",
      "7\n",
      "[[5848    1   16    3    3    9   18    6   10    9]\n",
      " [   1 6659   20   11    3    6    2   19   15    6]\n",
      " [  16   21 5786   29   15    4   16   33   33    5]\n",
      " [   8    2   51 5894    3   58    3   23   56   33]\n",
      " [   3    6    8    2 5671    1   26   10   10  105]\n",
      " [  18    9    6   63    8 5215   48   10   25   19]\n",
      " [  21    7    6    0   23   32 5817    1   11    0]\n",
      " [   2   19   31   14   18    7    0 6128    6   40]\n",
      " [   8   30   23   36   11   32   19    9 5650   33]\n",
      " [  17   11    2   31   83   19    1   50   25 5710]]\n",
      "0.9729666666666666\n",
      "8\n",
      "[[5831    2   17    3    2    9   23    4   18   14]\n",
      " [   1 6653   18   11    7    4    7   13   21    7]\n",
      " [  22   19 5799   24   16    5    9   32   26    6]\n",
      " [   3    7   49 5930    1   50    2   23   38   28]\n",
      " [   5    6   11    1 5692    1   25   18    8   75]\n",
      " [  15   12    8   57    5 5233   44    4   27   16]\n",
      " [  26    7    5    2   15   35 5813    0   15    0]\n",
      " [   3   22   32    9   16    5    0 6135    4   39]\n",
      " [   6   29   26   18   11   26   18    9 5678   30]\n",
      " [  17   10    2   26   75   18    3   44   33 5721]]\n",
      "0.97475\n",
      "9\n",
      "[[5828    1   18    4    7   11   23    3   15   13]\n",
      " [   0 6655   23    7    7    3    6   14   18    9]\n",
      " [  16   16 5797   27   16    3   11   32   32    8]\n",
      " [   8    6   53 5898    1   55    2   25   57   26]\n",
      " [   1    8   12    0 5683    2   27    8    9   92]\n",
      " [  15    8    5   71    8 5220   43    3   27   21]\n",
      " [  19    7    6    0   16   37 5822    0   11    0]\n",
      " [   2   17   39    8   20    3    0 6121   11   44]\n",
      " [   8   22   22   41   15   28    8   12 5665   30]\n",
      " [  16   12    4   26   67   22    2   49   37 5714]]\n",
      "0.9733833333333334\n"
     ]
    }
   ],
   "source": [
    "nn = model(x_train,y_train,10,alpha=0.001)\n",
    "layer1 = layer(784,196)\n",
    "layer2 = layer(196,58)\n",
    "layer3 = layer(58,10,activation='softmax')\n",
    "nn.add(layer1)\n",
    "nn.add(layer2)\n",
    "nn.add(layer3)\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class1 = np.loadtxt('NLS/Class1.txt')\n",
    "# class2 = np.loadtxt('NLS/Class2.txt')\n",
    "# alldata = []\n",
    "# for i in class1:\n",
    "#     alldata.append((i,0))\n",
    "# for i in class2:\n",
    "#     alldata.append((i,1))\n",
    "# np.random.shuffle(alldata)\n",
    "# train = alldata[:int(len(alldata)*0.75)]\n",
    "# test = alldata[int(len(alldata)*0.75):]\n",
    "# x_train = []\n",
    "# x_test = []\n",
    "# y_train = []\n",
    "# y_test = []\n",
    "\n",
    "# for i in train:\n",
    "#     x_train.append(i[0])\n",
    "#     y_train.append(i[1])\n",
    "\n",
    "# for i in test:\n",
    "#     x_test.append(i[0])\n",
    "#     y_test.append(i[1])\n",
    "\n",
    "# x_train = np.asarray(x_train)\n",
    "# y_train = np.asarray(y_train)\n",
    "# x_train = np.asarray(x_test)\n",
    "# y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = model(x_train,y_train,2)\n",
    "# layer1 = layer(2,8)\n",
    "# layer2 = layer(8,6)\n",
    "# layer3 = layer(6,2)\n",
    "# layer4 = layer(2,1,activation='sigmoid')\n",
    "# nn.add(layer1)\n",
    "# nn.add(layer2)\n",
    "# nn.add(layer3)\n",
    "# nn.add(layer4)\n",
    "# nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
