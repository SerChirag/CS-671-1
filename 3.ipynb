{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self,inputs,units,activation='relu',use_bias=True,kernel_initializer=None,bias_regularizer=False,dropout=False):\n",
    "        self.inputs = inputs\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "#         self.kernel_regularizer=kernel_regularizer\n",
    "        self.bias_regularizer=bias_regularizer\n",
    "        self.dropout = dropout\n",
    "        self.w = np.random.rand(self.units,self.inputs) * 0.001\n",
    "        self.b = np.random.rand(self.units,1)\n",
    "        self.inward = 0\n",
    "        self.z = 0\n",
    "        self.a = 0\n",
    "        self.dw = 0\n",
    "        self.db = 0\n",
    "        self.da = 0\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        #applying the sigmoid function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_d(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        x = x - np.mean(x)\n",
    "#         print x\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum()\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        #applying the tanh function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh_d(self, x):\n",
    "        return (1 + x) * (1 - x)\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x) \n",
    "    \n",
    "    def relu_d(self,x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def update(self,alpha):\n",
    "#         print \"W = \",self.w\n",
    "#         print \"b = \",self.b\n",
    "#         print \"dW = \",self.dw\n",
    "#         print \"db = \",self.db\n",
    "        self.w += alpha * self.dw\n",
    "        self.b += alpha * self.db\n",
    "        \n",
    "    \n",
    "    def forward(self,inward):\n",
    "        inward = inward.reshape((inward.shape[0],1))\n",
    "        self.inward = inward\n",
    "        self.z = np.dot(self.w,inward) \n",
    "        self.a = self.z\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            self.a = self.sigmoid(self.a)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.a = self.tanh(self.a)\n",
    "        elif(self.activation == 'relu'):\n",
    "            self.a = self.relu(self.a)\n",
    "        elif(self.activation == 'softmax'):\n",
    "            self.a = self.softmax(self.a)\n",
    "        else:\n",
    "            pass\n",
    "        return self.a\n",
    "            \n",
    "    def backward(self,a):\n",
    "#         print a.shape\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            self.dz = np.multiply(a,self.sigmoid(self.z))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.dz = np.multiply(a,self.tanh(self.z))\n",
    "        elif(self.activation == 'relu'):\n",
    "            self.dz = np.multiply(a,self.relu(self.z))\n",
    "        elif(self.activation == 'softmax'):\n",
    "            self.dz = a\n",
    "        else:\n",
    "            self.dz = 0 * a\n",
    "            \n",
    "        self.dw = np.dot(self.dz, self.inward.T)\n",
    "        self.db = np.sum(self.dz, axis=1, keepdims=True)\n",
    "        next_a = np.dot(self.w.T, self.dz)\n",
    "        return next_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self,data,data_label,accuracy=0.9,alpha=0.005,):\n",
    "        self.layers = []\n",
    "        self.alpha = alpha\n",
    "        self.accuracy = accuracy\n",
    "        self.data = data\n",
    "        self.data_label = data_label \n",
    "        self.cm = np.zeros((self.data_label.shape[-1],self.data_label.shape[-1]),dtype=int)\n",
    "        self.number_of_class = self.data_label.shape[-1]\n",
    "        \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def train(self):\n",
    "        i = 0\n",
    "        while(i < 100):\n",
    "            print i\n",
    "            self.cm = np.zeros((self.data_label.shape[-1],self.data_label.shape[-1]),dtype=int)\n",
    "            self.do_epoch()\n",
    "            self.get_accuracy()\n",
    "            i+=1\n",
    "#             print self.layers[0].dw\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        print self.cm\n",
    "        print np.trace(self.cm)/(1.0*np.sum(self.cm))\n",
    "\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update(self.alpha)\n",
    "    \n",
    "    def get_cost(self,y,y_label):\n",
    "        y_label = y_label.reshape((y_label.shape[0],1))\n",
    "#         print y_label,y\n",
    "        if(self.number_of_class > 2):\n",
    "            return (y_label - y)\n",
    "        else:\n",
    "            cost = (np.multiply(y, np.log(y_label)) + np.multiply(1 - y, np.log(1 - y_label)))\n",
    "            return cost\n",
    "    \n",
    "    def predict(self,sample):\n",
    "        for layer in self.layers:\n",
    "            sample = layer.forward(sample)\n",
    "        return sample.argmax()\n",
    "    \n",
    "    def do_epoch(self):\n",
    "        total_cost = 0\n",
    "        for j in range(len(self.data)):\n",
    "            y_label = self.data_label[j]\n",
    "            x = self.data[j]\n",
    "            y = self.forprop(x)\n",
    "            self.cm[y_label.argmax()][y.argmax()] += 1\n",
    "            loss = self.get_cost(y,y_label)\n",
    "#             print \"loss = \",loss\n",
    "            self.backprop(loss)\n",
    "            self.update()\n",
    "#             total_cost += loss\n",
    "            \n",
    "    def forprop(self,sample):\n",
    "        for layer in self.layers:\n",
    "#             print sample\n",
    "            sample = layer.forward(sample)\n",
    "                  \n",
    "        return sample       \n",
    "    \n",
    "    def backprop(self,loss):\n",
    "        sample = loss\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            sample = self.layers[i].backward(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train_label),(x_test, y_test_label) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape([60000,784])\n",
    "x_test = x_test.reshape([10000,784])\n",
    "y_train = np.zeros((y_train_label.shape[0], 10))\n",
    "y_train[np.arange(y_train_label.shape[0]), y_train_label] = 1\n",
    "y_test = np.zeros((y_test_label.shape[0], 10))\n",
    "y_test[np.arange(y_test_label.shape[0]), y_test_label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[5903    0   16    3    0    0    0    0    1    0]\n",
      " [6719    0   21    1    0    0    0    0    1    0]\n",
      " [5942    0   15    0    0    0    0    0    1    0]\n",
      " [6115    0   15    1    0    0    0    0    0    0]\n",
      " [5823    0   14    4    0    0    0    0    1    0]\n",
      " [5409    0   11    0    0    0    0    0    1    0]\n",
      " [5901    0   16    1    0    0    0    0    0    0]\n",
      " [6251    0   13    1    0    0    0    0    0    0]\n",
      " [5834    0   16    1    0    0    0    0    0    0]\n",
      " [5925    0   21    2    0    0    0    0    1    0]]\n",
      "0.09865\n",
      "1\n",
      "[[5549    0   72   86    0  122   60    2   32    0]\n",
      " [6485    0   68   58    0   47   61    0   23    0]\n",
      " [5619    0   70   99    0   87   60    2   21    0]\n",
      " [5809    0   62   78    0  100   51    0   31    0]\n",
      " [5606    0   58   54    0   52   49    0   23    0]\n",
      " [5141    0   56   69    0   84   43    1   27    0]\n",
      " [5616    0   64   69    0  102   48    1   18    0]\n",
      " [5943    0   66   67    0  103   62    0   24    0]\n",
      " [5481    0   76   91    0  113   63    2   25    0]\n",
      " [5664    0   72   76    0   76   41    2   18    0]]\n",
      "0.09756666666666666\n",
      "2\n",
      "[[5551    0   21   80    0  229   18    4   20    0]\n",
      " [2173  771   71 2636    0  519    2    0  570    0]\n",
      " [2168   88  216 2339    0  676   25    2  444    0]\n",
      " [1475   52  133 3468    0  733   47    0  223    0]\n",
      " [2660   28   95 1824    0  710   12    0  512    1]\n",
      " [1923   20   80 1262    0 1889   34    0  213    0]\n",
      " [2516   25  132 1816    0  967   35    1  426    0]\n",
      " [2682   56  101 1955    0 1042   16    0  413    0]\n",
      " [1469   62  140 2464    0 1090   31    0  594    1]\n",
      " [2374   23   89 2002    0  897   18    0  546    0]]\n",
      "0.20873333333333333\n",
      "3\n",
      "[[5653    0   29   10    1   74  121    6   27    2]\n",
      " [ 141 5578  120  329    5  235   31    0  290   13]\n",
      " [ 885   47 3666  405   30  203  420    1  279   22]\n",
      " [ 196   23  169 4915    5  548   38    8  197   32]\n",
      " [1647   19  221  285  709 1024  394   44 1092  407]\n",
      " [ 383   23   76  423   11 4030  118    9  295   53]\n",
      " [1039    7  300  102   45  610 3583    7  209   16]\n",
      " [2329   39  310  585  189 1375   69  174  937  258]\n",
      " [ 515   70  258  572   62 1831  150    5 2279  109]\n",
      " [1300   20  160  500  471 1540  106  105 1305  442]]\n",
      "0.51715\n",
      "4\n",
      "[[5734    1   28    6    2   53   76   12    8    3]\n",
      " [ 306 6031   32   53    5  130   32    5  145    3]\n",
      " [2675   17 2170  187   11  367  262   35  223   11]\n",
      " [ 975    8  150 3776    5  878   73  106  149   11]\n",
      " [2632   12   99   32 1443  873  237   50  188  276]\n",
      " [1001   14   39  194   45 3729  131   16  236   16]\n",
      " [2280    4  169   30   14  998 2290    4  120    9]\n",
      " [3165   21  116   93   49  292   91 2288   66   84]\n",
      " [1340   46  110  128   20 1190  105   19 2857   36]\n",
      " [3321   16   61  102  226 1284  155  161  314  309]]\n",
      "0.51045\n",
      "5\n",
      "[[5705    0   32   16    0   89   49   17   15    0]\n",
      " [ 298 6115    1  147    1   37    5    1  137    0]\n",
      " [3553   16 1329  470    3  213  120    6  248    0]\n",
      " [ 751    6   37 4767    0  407   18   30  115    0]\n",
      " [4889   10   31  151  194  195  121    5  246    0]\n",
      " [1622    5   41  723   25 2429   47   25  502    2]\n",
      " [2110    3   22   30    0  169 3480    1  103    0]\n",
      " [5261   24   95  363    3  308   94   30   87    0]\n",
      " [1513   47   57  332    6  324   28    0 3544    0]\n",
      " [4748   17   42  349    2  391   39    0  361    0]]\n",
      "0.4598833333333333\n",
      "6\n",
      "[[5660    1   23   43    0   81   53   28   34    0]\n",
      " [ 300 6245    3   46    0   19   12    0  117    0]\n",
      " [4217   27  962  259    0   57  160    0  276    0]\n",
      " [1805   47   16 3797    7  248   28   12  171    0]\n",
      " [5258   14   14   97    3   78   96   24  258    0]\n",
      " [1760   22    9  465   17 2581   72   13  472   10]\n",
      " [1430    7   15   27    0   74 4269    7   89    0]\n",
      " [5192   30   14  161    0   95   56  612  105    0]\n",
      " [1740   67    3  205    0  174   31    0 3631    0]\n",
      " [5036   18   12  206    1  226   38    9  403    0]]\n",
      "0.46266666666666667\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-759b4673578a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-b42f63a916a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-b42f63a916a0>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#             print \"loss = \",loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#             total_cost += loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-b42f63a916a0>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-556b30528ead>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mnext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = model(x_train,y_train,0.6)\n",
    "layer1 = layer(784,196)\n",
    "layer2 = layer(196,58)\n",
    "layer3 = layer(58,10,activation='softmax')\n",
    "nn.add(layer1)\n",
    "nn.add(layer2)\n",
    "nn.add(layer3)\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5 -0.5  0.5  1.5]\n",
      "[0.0320586  0.08714432 0.23688282 0.64391426]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
